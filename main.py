"""
ë©”ì¸ ì‹¤í–‰ íŒŒì¼: WAV íŒŒì¼ ì „ì²˜ë¦¬ + ONNX ë¶„ë¥˜ ë¶„ì„ì„ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰
"""

import os
import sys
import json
import csv
import glob
from datetime import datetime
from pathlib import Path

# 1ë‹¨ê³„: .pt íŒŒì¼ ìƒì„± (audio_preprocessing.py)
from audio_preprocessing import process_wav_file, load_model
from resample import init_resampler

# 2ë‹¨ê³„: .pt íŒŒì¼ ë¶„ì„ (integrated_analysis.py)  
from integrated_analysis import process_pt_files_with_classification

def main_pipeline(wav_file_path, target_parts, onnx_model_base_path="ResNet18_onnx", device_name="machine_001"):
    """
    ì™„ì „í•œ íŒŒì´í”„ë¼ì¸: WAV íŒŒì¼ â†’ .pt íŒŒì¼ ìƒì„± â†’ ê° ë¶€í’ˆë³„ ì „ìš© ONNX ëª¨ë¸ë¡œ ë¶„ë¥˜ ë¶„ì„
    
    Args:
        wav_file_path: ì…ë ¥ WAV íŒŒì¼ ê²½ë¡œ
        target_parts: ë¶„ì„í•  ë¶€í’ˆ ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: ['fan', 'pump'])
        onnx_model_base_path: ë¶€í’ˆë³„ ONNX ëª¨ë¸ë“¤ì´ ì €ì¥ëœ í´ë” ê²½ë¡œ
        device_name: ì¥ì¹˜ëª…
    
    Returns:
        dict: ìµœì¢… ë¶„ì„ ê²°ê³¼
    """
    
    print("ğŸš€ í†µí•© ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì‹œì‘")
    print("="*60)
    
    try:
        # === 1ë‹¨ê³„: .pt íŒŒì¼ ìƒì„± ===
        print("ğŸ“‹ 1ë‹¨ê³„: WAV íŒŒì¼ì—ì„œ .pt íŒŒì¼ ìƒì„±")
        print(f"ğŸ“ ì…ë ¥ íŒŒì¼: {wav_file_path}")
        print(f"ğŸ¯ ëŒ€ìƒ ë¶€í’ˆ: {target_parts}")
        
        # Demucs ëª¨ë¸ ë¡œë“œ
        print("ğŸ”§ Demucs ëª¨ë¸ ë¡œë”© ì¤‘...")
        model, source_names = load_model()
        init_resampler(model.samplerate)
        
        # .pt íŒŒì¼ ìƒì„±
        generated_files = process_wav_file(model, source_names, wav_file_path, target_parts=target_parts)
        
        if not generated_files:
            raise ValueError("âŒ .pt íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        print(f"âœ… 1ë‹¨ê³„ ì™„ë£Œ: {len(generated_files)}ê°œ .pt íŒŒì¼ ìƒì„±")
        for file_path in generated_files:
            print(f"  ğŸ“„ {file_path}")
        
        # === 2ë‹¨ê³„: .pt íŒŒì¼ ë¶„ì„ ===
        print(f"\nğŸ“‹ 2ë‹¨ê³„: ê° ë¶€í’ˆë³„ ì „ìš© ONNX ëª¨ë¸ë¡œ ë¶„ë¥˜ ë¶„ì„")
        print(f"ğŸ¤– ONNX ëª¨ë¸ í´ë”: {onnx_model_base_path}")
        
        analysis_results = process_pt_files_with_classification(
            pt_files=generated_files,
            onnx_model_base_path=onnx_model_base_path,
            device_name=device_name
        )
        
        print(f"âœ… 2ë‹¨ê³„ ì™„ë£Œ: {analysis_results['total_parts']}ê°œ ë¶€í’ˆ ë¶„ì„")
        
        # === ìµœì¢… ê²°ê³¼ í†µí•© ===
        final_result = {
            "pipeline_info": {
                "input_wav_file": wav_file_path,
                "target_parts": target_parts,
                "generated_pt_files": generated_files,
                "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            },
            "analysis_results": analysis_results
        }
        
        return final_result
        
    except Exception as e:
        print(f"âŒ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise

def print_final_results(results):
    """ê²°ê³¼ë¥¼ ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥í•©ë‹ˆë‹¤."""
    
    print("\n" + "="*60)
    print("ğŸ¯ ìµœì¢… í†µí•© ë¶„ì„ ê²°ê³¼")
    print("="*60)
    
    # íŒŒì´í”„ë¼ì¸ ì •ë³´
    pipeline_info = results["pipeline_info"]
    print(f"ğŸ“ ì…ë ¥ íŒŒì¼: {pipeline_info['input_wav_file']}")
    print(f"ğŸ¯ ë¶„ì„ ëŒ€ìƒ: {pipeline_info['target_parts']}")
    print(f"ğŸ“„ ìƒì„±ëœ .pt íŒŒì¼: {len(pipeline_info['generated_pt_files'])}ê°œ")
    print(f"â° ì²˜ë¦¬ ì‹œê°„: {pipeline_info['timestamp']}")
    
    # ë¶„ì„ ê²°ê³¼
    analysis = results["analysis_results"]
    print(f"\nğŸ­ ì¥ì¹˜: {analysis['device_name']}")
    print(f"ğŸ“Š ë¶„ì„ ë¶€í’ˆ ìˆ˜: {analysis['total_parts']}")
    print(f"âš ï¸ ì´ìƒ ê°ì§€ ë¶€í’ˆ: {analysis['anomaly_count']}")
    
    print("\nğŸ“‹ ìƒì„¸ ê²°ê³¼:")
    for result in analysis['results']:
        status = "ğŸš¨ ì´ìƒ" if result['anomaly_detected'] else "âœ… ì •ìƒ"
        print(f"  {result['part_name']}: {status} "
              f"(í™•ë¥ : {result['anomaly_probability']:.3f})")
    
    # ìƒì„±ëœ íŒŒì¼ë“¤
    print(f"\nğŸ“„ ìƒì„±ëœ .pt íŒŒì¼ë“¤:")
    for file_path in pipeline_info['generated_pt_files']:
        print(f"  ğŸ“„ {file_path}")

def save_results_to_json(results, output_filename=None):
    """ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    
    if output_filename is None:
        device_name = results["analysis_results"]["device_name"]
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_filename = f"pipeline_result_{device_name}_{timestamp}.json"
    
    import json
    with open(output_filename, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ ê²°ê³¼ê°€ {output_filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    return output_filename

def load_metadata(metadata_path):
    """metadata.json íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    try:
        with open(metadata_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        print(f"âŒ metadata.json ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None

def find_test_samples(test_dir):
    """test ë””ë ‰í† ë¦¬ì—ì„œ sample í´ë”ë“¤ì„ ì°¾ìŠµë‹ˆë‹¤."""
    sample_pattern = os.path.join(test_dir, "sample*")
    sample_dirs = glob.glob(sample_pattern)
    return sorted(sample_dirs)

def process_batch_samples(test_dir, target_parts=None, onnx_model_base_path="ResNet18_onnx"):
    """
    test ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  sample í´ë”ë¥¼ ë°°ì¹˜ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    
    Args:
        test_dir: test ë””ë ‰í† ë¦¬ ê²½ë¡œ
        target_parts: ë¶„ì„í•  ë¶€í’ˆ ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ metadataì—ì„œ ì¶”ì¶œ)
        onnx_model_base_path: ë¶€í’ˆë³„ ONNX ëª¨ë¸ë“¤ì´ ì €ì¥ëœ í´ë” ê²½ë¡œ
    
    Returns:
        list: ê° ìƒ˜í”Œì˜ ì²˜ë¦¬ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    """
    
    print("ğŸš€ ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘")
    print("="*60)
    
    # ëª¨ë¸ ë¡œë“œ (í•œ ë²ˆë§Œ)
    print("ğŸ”§ Demucs ëª¨ë¸ ë¡œë”© ì¤‘...")
    model, source_names = load_model()
    init_resampler(model.samplerate)
    
    # ìƒ˜í”Œ í´ë”ë“¤ ì°¾ê¸°
    sample_dirs = find_test_samples(test_dir)
    
    if not sample_dirs:
        print(f"âŒ {test_dir}ì—ì„œ sample í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return []
    
    print(f"ğŸ“ ë°œê²¬ëœ ìƒ˜í”Œ: {len(sample_dirs)}ê°œ")
    for sample_dir in sample_dirs:
        print(f"  ğŸ“‚ {os.path.basename(sample_dir)}")
    
    all_results = []
    
    for i, sample_dir in enumerate(sample_dirs):
        sample_name = os.path.basename(sample_dir)
        print(f"\nğŸ”„ ì²˜ë¦¬ ì¤‘: {sample_name} ({i+1}/{len(sample_dirs)})")
        
        try:
            # í•„ìˆ˜ íŒŒì¼ í™•ì¸
            mixture_path = os.path.join(sample_dir, "mixture.wav")
            metadata_path = os.path.join(sample_dir, "metadata.json")
            
            if not os.path.exists(mixture_path):
                print(f"âŒ mixture.wav ì—†ìŒ: {sample_dir}")
                continue
                
            if not os.path.exists(metadata_path):
                print(f"âŒ metadata.json ì—†ìŒ: {sample_dir}")
                continue
            
            # metadata ë¡œë“œ
            metadata = load_metadata(metadata_path)
            if not metadata:
                continue
            
            # target_parts ê²°ì • (metadataì—ì„œ ì¶”ì¶œí•˜ê±°ë‚˜ ì‚¬ìš©ì ì§€ì •)
            if target_parts is None and 'components' in metadata:
                current_target_parts = list(metadata['components'].keys())
            else:
                current_target_parts = target_parts or ["fan", "pump", "slider", "bearing", "gearbox"]
            
            print(f"ğŸ¯ ë¶„ì„ ëŒ€ìƒ: {current_target_parts}")
            
            # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
            result = main_pipeline(
                wav_file_path=mixture_path,
                target_parts=current_target_parts,
                onnx_model_base_path=onnx_model_base_path,
                device_name=sample_name
            )
            
            # metadata ì •ë³´ ì¶”ê°€
            result["metadata"] = metadata
            result["sample_info"] = {
                "sample_dir": sample_dir,
                "sample_name": sample_name
            }
            
            all_results.append(result)
            print(f"âœ… {sample_name} ì²˜ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ {sample_name} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            error_result = {
                "sample_info": {
                    "sample_name": sample_name,
                    "sample_dir": sample_dir
                },
                "error": str(e),
                "metadata": None,
                "pipeline_info": None,
                "analysis_results": None
            }
            all_results.append(error_result)
    
    return all_results

def save_results_to_csv(batch_results, csv_filename=None):
    """
    ë°°ì¹˜ ì²˜ë¦¬ ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    
    Args:
        batch_results: process_batch_samplesì˜ ê²°ê³¼
        csv_filename: ì €ì¥í•  CSV íŒŒì¼ëª…
    
    Returns:
        str: ì €ì¥ëœ CSV íŒŒì¼ ê²½ë¡œ
    """
    
    if csv_filename is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        csv_filename = f"batch_analysis_results_{timestamp}.csv"
    
    # CSV í—¤ë” ì •ì˜
    fieldnames = [
        'sample_name',
        'split',                  # test/train/val êµ¬ë¶„
        'part_name', 
        'ground_truth',           # metadataì—ì„œ (normal/abnormal)
        'predicted',              # AI ëª¨ë¸ ê²°ê³¼ (normal/abnormal)
        'prediction_probability', # AI ëª¨ë¸ í™•ë¥ 
        'correct',                # ì •ë‹µ ì—¬ë¶€
        'source_file',            # ì›ë³¸ ì†ŒìŠ¤ íŒŒì¼ ê²½ë¡œ
        'ground_truth_rms_db',    # ì›ë³¸ RMS ê°’
        'mixture_file_path',
        'pt_file_path',
        'processing_timestamp'
    ]
    
    csv_data = []
    
    for result in batch_results:
        if "error" in result:
            # ì—ëŸ¬ ì¼€ì´ìŠ¤
            sample_info = result.get('sample_info', {})
            csv_data.append({
                'sample_name': sample_info.get('sample_name', 'unknown'),
                'split': 'unknown',
                'part_name': 'ERROR',
                'ground_truth': 'unknown',
                'predicted': 'error',
                'prediction_probability': 0.0,
                'correct': False,
                'source_file': '',
                'ground_truth_rms_db': 0.0,
                'mixture_file_path': sample_info.get('sample_dir', ''),
                'pt_file_path': '',
                'processing_timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            })
            continue
        
        # ì •ìƒ ì²˜ë¦¬ ì¼€ì´ìŠ¤
        sample_info = result.get('sample_info', {})
        sample_name = sample_info.get('sample_name', 'unknown')
        metadata = result.get('metadata', {})
        pipeline_info = result.get('pipeline_info', {})
        analysis_results = result.get('analysis_results', {})
        
        mixture_path = pipeline_info.get('input_wav_file', '')
        timestamp = pipeline_info.get('timestamp', '')
        generated_files = pipeline_info.get('generated_pt_files', [])
        
        # metadataì—ì„œ split ì •ë³´ ì¶”ì¶œ
        split_info = metadata.get('split', 'unknown')
        
        # ê° ë¶€í’ˆë³„ë¡œ í–‰ ìƒì„±
        for analysis in analysis_results.get('results', []):
            part_name = analysis.get('part_name', 'unknown')
            
            # Ground truth ì¶”ì¶œ (ìƒˆë¡œìš´ metadata êµ¬ì¡°)
            component_info = metadata.get('components', {}).get(part_name, {})
            ground_truth = component_info.get('status', 'unknown')
            source_file = component_info.get('source_file', '')
            rms_db = component_info.get('rms_db', 0.0)
            
            # ì˜ˆì¸¡ ê²°ê³¼
            predicted_bool = analysis.get('anomaly_detected', False)
            predicted = 'abnormal' if predicted_bool else 'normal'
            probability = analysis.get('anomaly_probability', 0.0)
            
            # ì •ë‹µ ì—¬ë¶€ ê³„ì‚°
            # ground_truthê°€ 'faulty'ë©´ 'abnormal'ë¡œ ê°„ì£¼
            gt_for_compare = 'abnormal' if ground_truth == 'faulty' else ground_truth
            correct = (gt_for_compare == predicted)
            
            # í•´ë‹¹ ë¶€í’ˆì˜ .pt íŒŒì¼ ê²½ë¡œ ì°¾ê¸°
            pt_file_path = ''
            for pt_file in generated_files:
                if part_name in pt_file:
                    pt_file_path = pt_file
                    break
            
            csv_data.append({
                'sample_name': sample_name,
                'split': split_info,
                'part_name': part_name,
                'ground_truth': ground_truth,
                'predicted': predicted,
                'prediction_probability': round(probability, 3),
                'correct': correct,
                'source_file': source_file,
                'ground_truth_rms_db': round(rms_db, 3),
                'mixture_file_path': mixture_path,
                'pt_file_path': pt_file_path,
                'processing_timestamp': timestamp
            })
    
    # CSV íŒŒì¼ ì €ì¥
    with open(csv_filename, 'w', newline='', encoding='utf-8') as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(csv_data)
    
    print(f"\nğŸ“Š CSV ê²°ê³¼ ì €ì¥: {csv_filename}")
    print(f"ğŸ“ˆ ì´ {len(csv_data)}ê°œ í–‰ ì €ì¥")
    
    # ê°„ë‹¨í•œ í†µê³„ ì¶œë ¥
    if csv_data:
        total_predictions = len([row for row in csv_data if row['part_name'] != 'ERROR'])
        correct_predictions = len([row for row in csv_data if row['correct'] == True])
        
        if total_predictions > 0:
            accuracy = (correct_predictions / total_predictions) * 100
            print(f"ğŸ¯ ì „ì²´ ì •í™•ë„: {accuracy:.2f}% ({correct_predictions}/{total_predictions})")
    
    return csv_filename

# === ë©”ì¸ ì‹¤í–‰ ë¶€ë¶„ ===
if __name__ == "__main__":
    # ì„¤ì •ê°’ë“¤
    
    # ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬ ëª¨ë“œ
    SINGLE_MODE = False  # Falseë¡œ ì„¤ì •í•˜ë©´ ë°°ì¹˜ ì²˜ë¦¬ ëª¨ë“œ
    
    if SINGLE_MODE:
        # === ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬ ===
        WAV_FILE = "test01/mixture.wav"  # ì…ë ¥ WAV íŒŒì¼
        TARGET_PARTS = ["fan", "pump"]   # ë¶„ì„í•  ë¶€í’ˆë“¤
        ONNX_MODEL_BASE_PATH = "ResNet18_onnx"  # ë¶€í’ˆë³„ ONNX ëª¨ë¸ë“¤ì´ ì €ì¥ëœ í´ë”
        DEVICE_NAME = "machine_001"      # ì¥ì¹˜ëª…
        
        try:
            # í†µí•© íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
            results = main_pipeline(
                wav_file_path=WAV_FILE,
                target_parts=TARGET_PARTS,
                onnx_model_base_path=ONNX_MODEL_BASE_PATH,
                device_name=DEVICE_NAME
            )
            
            # ê²°ê³¼ ì¶œë ¥
            print_final_results(results)
            
            # JSON íŒŒì¼ë¡œ ì €ì¥
            save_results_to_json(results)
            
            print("\nğŸ‰ ëª¨ë“  ê³¼ì •ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            
        except Exception as e:
            print(f"âŒ ë©”ì¸ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()
            sys.exit(1)
    
    else:
        # === ë°°ì¹˜ ì²˜ë¦¬ ëª¨ë“œ ===
        TEST_DIR = "test2"  # test ë””ë ‰í† ë¦¬ ê²½ë¡œ
        TARGET_PARTS = None  # Noneì´ë©´ metadataì—ì„œ ìë™ ì¶”ì¶œ
        ONNX_MODEL_BASE_PATH = "ResNet18_onnx"  # ë¶€í’ˆë³„ ONNX ëª¨ë¸ë“¤ì´ ì €ì¥ëœ í´ë”
        
        try:
            print("ğŸš€ ë°°ì¹˜ ì²˜ë¦¬ ëª¨ë“œ ì‹œì‘")
            print(f"ğŸ“‚ í…ŒìŠ¤íŠ¸ ë””ë ‰í† ë¦¬: {TEST_DIR}")
            
            # ë°°ì¹˜ ì²˜ë¦¬ ì‹¤í–‰
            batch_results = process_batch_samples(
                test_dir=TEST_DIR,
                target_parts=TARGET_PARTS,
                onnx_model_base_path=ONNX_MODEL_BASE_PATH
            )
            
            if not batch_results:
                print("âŒ ì²˜ë¦¬í•  ìƒ˜í”Œì´ ì—†ìŠµë‹ˆë‹¤.")
                sys.exit(1)
            
            # ê²°ê³¼ë¥¼ CSVë¡œ ì €ì¥
            csv_file = save_results_to_csv(batch_results)
            
            # ì „ì²´ ë°°ì¹˜ ê²°ê³¼ë¥¼ JSONìœ¼ë¡œë„ ì €ì¥
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            batch_json_file = f"batch_results_{timestamp}.json"
            
            with open(batch_json_file, 'w', encoding='utf-8') as f:
                json.dump(batch_results, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ“Š ë°°ì¹˜ ê²°ê³¼ JSON ì €ì¥: {batch_json_file}")
            print(f"ğŸ“ˆ ì²˜ë¦¬ ì™„ë£Œ: {len(batch_results)}ê°œ ìƒ˜í”Œ")
            print("\nğŸ‰ ë°°ì¹˜ ì²˜ë¦¬ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            
        except Exception as e:
            print(f"âŒ ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()
            sys.exit(1)
